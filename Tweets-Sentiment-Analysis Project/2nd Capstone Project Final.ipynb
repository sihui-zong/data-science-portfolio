{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Description\n",
    "The goal of this project is to build a model that can do sentiment analysis on Twitter messages. Sentiment analysis is a method for gauging attitude and/or opinions towards particular topics expressed in text. The analysis in this project, to be specific,  is to determine whether the Twitter users’ attitude towards products from two particular brands,  Apple and Google, is positive or negative.  Brands can use the results to monitor their reputation across social media. Furthermore, companies and brands can make improvements on products perfectly met customers’ demands.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import string\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from time import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'''C:\\Users\\mia\\files\\drive-download-20190215T130906Z-001\\judge-1377884607_tweet_product_company.csv''',encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "1              iPad or iPhone App   \n",
       "2                            iPad   \n",
       "3              iPad or iPhone App   \n",
       "4                          Google   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                   Negative emotion  \n",
       "1                                   Positive emotion  \n",
       "2                                   Positive emotion  \n",
       "3                                   Negative emotion  \n",
       "4                                   Positive emotion  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9093 entries, 0 to 9092\n",
      "Data columns (total 3 columns):\n",
      "tweet_text                                            9092 non-null object\n",
      "emotion_in_tweet_is_directed_at                       3291 non-null object\n",
      "is_there_an_emotion_directed_at_a_brand_or_product    9093 non-null object\n",
      "dtypes: object(3)\n",
      "memory usage: 213.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the column names for convenience\n",
    "df.columns = ['tweets','brand','emotion']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Just from the first five rows, it looks like this data set contains tweets on more than one brand's products. Let's discover how many different brands these tweets are about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tweets</th>\n",
       "      <td>9092</td>\n",
       "      <td>9065</td>\n",
       "      <td>RT @mention Marissa Mayer: Google Will Connect...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand</th>\n",
       "      <td>3291</td>\n",
       "      <td>9</td>\n",
       "      <td>iPad</td>\n",
       "      <td>946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emotion</th>\n",
       "      <td>9093</td>\n",
       "      <td>4</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>5389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        count unique                                                top  freq\n",
       "tweets   9092   9065  RT @mention Marissa Mayer: Google Will Connect...     5\n",
       "brand    3291      9                                               iPad   946\n",
       "emotion  9093      4                 No emotion toward brand or product  5389"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The target brand column has 9 unique values. Take a closer look at those 9 unique values, I realize that it does not mean 9 unique brands rather two main brands and others:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['iPhone', 'iPad or iPhone App', 'iPad', 'Google', nan, 'Android',\n",
       "       'Apple', 'Android App', 'Other Google product or service',\n",
       "       'Other Apple product or service'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.brand.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Android</th>\n",
       "      <td>78</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Android App</th>\n",
       "      <td>81</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Apple</th>\n",
       "      <td>661</td>\n",
       "      <td>661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Google</th>\n",
       "      <td>430</td>\n",
       "      <td>430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other Apple product or service</th>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other Google product or service</th>\n",
       "      <td>293</td>\n",
       "      <td>293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iPad</th>\n",
       "      <td>946</td>\n",
       "      <td>946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iPad or iPhone App</th>\n",
       "      <td>470</td>\n",
       "      <td>470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iPhone</th>\n",
       "      <td>297</td>\n",
       "      <td>297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 tweets  emotion\n",
       "brand                                           \n",
       "Android                              78       78\n",
       "Android App                          81       81\n",
       "Apple                               661      661\n",
       "Google                              430      430\n",
       "Other Apple product or service       35       35\n",
       "Other Google product or service     293      293\n",
       "iPad                                946      946\n",
       "iPad or iPhone App                  470      470\n",
       "iPhone                              297      297"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('brand').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### It is more clear see that most of the tweets are about products of service of brands, Google and Apple when the data is grouped by ‘brand’. The data set will be partitioned into two parts based on the different target brands. For those tweets which have 'Android' and 'Android App' as brand, one can't tell which specific brand's products they are about, so they will be dropped. The model will be built using  tweets about only Apple products. After the model is finally chosen, the data of Google products will be used to see whether the model can use on tweets about a different brand, in other words,  to see how generalized the model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Negative emotion', 'Positive emotion',\n",
       "       'No emotion toward brand or product', \"I can't tell\"], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.emotion.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Moreover, this data set has four emotions: ‘Negative emotion’, ‘Positive emotion’, ‘I can’t tell’, and ‘No emotion toward brand or product’. Only two are the ones that matter in this project -- positive emotion and negative emotion. Thus, the samples with emotions other than positive or negative will be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kepp tweets with only positive emotion or negative emotion.\n",
    "df = df.loc[df.emotion.isin(['Negative emotion','Positive emotion'])]\n",
    "\n",
    "# Create two new dataframes based on the brand the tweets are about \n",
    "# Only Keep tweets on two brands -- Apple and Google\n",
    "apple_df = df.loc[df.brand.isin(['iPhone', 'iPad or iPhone App', 'iPad','Apple','Other Apple product or service'])]\n",
    "google_df = df.loc[df.brand.isin(['Google','Other Google product or service'])]\n",
    "\n",
    "# Reset index of two new dataframes\n",
    "apple_df.reset_index(inplace=True,drop=True)\n",
    "google_df.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tweets</th>\n",
       "      <td>2337</td>\n",
       "      <td>2332</td>\n",
       "      <td>Oh. My. God. The #SXSW app for iPad is pure, u...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand</th>\n",
       "      <td>2337</td>\n",
       "      <td>5</td>\n",
       "      <td>iPad</td>\n",
       "      <td>918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emotion</th>\n",
       "      <td>2337</td>\n",
       "      <td>2</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>1949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        count unique                                                top  freq\n",
       "tweets   2337   2332  Oh. My. God. The #SXSW app for iPad is pure, u...     2\n",
       "brand    2337      5                                               iPad   918\n",
       "emotion  2337      2                                   Positive emotion  1949"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apple_df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notice the unique tweets are two less than the total tweets, which means there are duplicates. The duplicates will be dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tweets</th>\n",
       "      <td>2332</td>\n",
       "      <td>2332</td>\n",
       "      <td>Rockin an iPad 2 from the downtown Apple #SXSW...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand</th>\n",
       "      <td>2332</td>\n",
       "      <td>5</td>\n",
       "      <td>iPad</td>\n",
       "      <td>917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emotion</th>\n",
       "      <td>2332</td>\n",
       "      <td>2</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>1945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        count unique                                                top  freq\n",
       "tweets   2332   2332  Rockin an iPad 2 from the downtown Apple #SXSW...     1\n",
       "brand    2332      5                                               iPad   917\n",
       "emotion  2332      2                                   Positive emotion  1945"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop depulicate tweets\n",
    "apple_df = apple_df.loc[~apple_df.tweets.duplicated()]\n",
    "apple_df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tweets</th>\n",
       "      <td>697</td>\n",
       "      <td>695</td>\n",
       "      <td>RT @mention Marissa Mayer: Google Will Connect...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand</th>\n",
       "      <td>697</td>\n",
       "      <td>2</td>\n",
       "      <td>Google</td>\n",
       "      <td>414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emotion</th>\n",
       "      <td>697</td>\n",
       "      <td>2</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        count unique                                                top freq\n",
       "tweets    697    695  RT @mention Marissa Mayer: Google Will Connect...    3\n",
       "brand     697      2                                             Google  414\n",
       "emotion   697      2                                   Positive emotion  582"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Same thing with data about google products. Duplicates will be ignored as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tweets</th>\n",
       "      <td>695</td>\n",
       "      <td>695</td>\n",
       "      <td>Fantastico! RT @mention Marissa Mayer: Google ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand</th>\n",
       "      <td>695</td>\n",
       "      <td>2</td>\n",
       "      <td>Google</td>\n",
       "      <td>412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emotion</th>\n",
       "      <td>695</td>\n",
       "      <td>2</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        count unique                                                top freq\n",
       "tweets    695    695  Fantastico! RT @mention Marissa Mayer: Google ...    1\n",
       "brand     695      2                                             Google  412\n",
       "emotion   695      2                                   Positive emotion  580"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_df = google_df.loc[~google_df.tweets.duplicated()]\n",
    "google_df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, move to clean the tweets and convert them to be numerically representable. The URLs, Twitter usernames (@something), any numbers, any punctuations, and any special characters will be removed. After taking out the URLs and before taking out any punctuations, the contractions of words will be removed at first since the contractions can result in misinterpreting the meaning of a phrase, especially in the case of negations if the punctuation, apostrophe, is just simply being removed. And since the main goal to use the package, contractions, is to avoid the misinterpreting any negations. Thus, phrases, such as '' year's '', are not going to be expanded. For more information on this package, please see  [here](https://github.com/kootenpv/contractions). The technique used in the removals to match the URLs, usernames and so on is the regular expression library. After cleaning, the tweet messages will be tokenized and then join with a space between words. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "\n",
    "# Define a function that takes a series as argument cleans tweets\n",
    "def process_tweets(tweets):\n",
    "    # Remove URLs in tweets\n",
    "    clean_tweets = tweets.apply(\n",
    "        lambda x: re.sub(r'http?:\\/\\/.*[\\r\\n]*', ' ',x))\n",
    "    # Remove tweet user names\n",
    "    clean_tweets = clean_tweets.apply(lambda x: re.sub(r'@[\\w]*', ' ',x))\n",
    "    \n",
    "    # Expand contractions\n",
    "    clean_tweets = clean_tweets.apply(lambda x: contractions.fix(x))\n",
    "    \n",
    "    # Remove punctuations, numbers, and any special characters\n",
    "    clean_tweets = clean_tweets.apply(lambda x: re.sub(r'[^a-zA-Z\\s\\']',' ',x))\n",
    "    # Tokenization\n",
    "    tokens = clean_tweets.apply(lambda x:x.split())\n",
    "    \n",
    "    return [' '.join(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Furthermore, I create two functions to normalize the tweets even more by stemming words or lemmatizating words. The goal of both stemming and lemmatization is to get the root forms of derived words, but they differ in their approaches. Stemming usually just chops ff the ends of words. Lemmatization uses the morphological analysis of words. Ronald Wahome claims that the above two techniques may not work so well because they essentially shorten words to their base words and the Twitter messages are short messages by design. I still would like to try the two methods and to see whether they would make a distinct difference on improving my model. There many different  implementations for stemming and lemmatization in python, I use the LancasterStemmer and the WordNetLemmatizer from the NLTK library. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(tweet):\n",
    "    # The argument 'tweet' is a string list\n",
    "    \n",
    "    words = tweet.split()\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = [stemmer.stem(word) for word in words]\n",
    "    return ' '.join(stems)\n",
    "\n",
    "def lemmatize_words(tweet):\n",
    "    # The argument 'tweet' is a string lis\n",
    "    \n",
    "    words = tweet.split()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    verb_lemmas = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
    "    noun_lemmas = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    return ' '.join(verb_lemmas),' '.join(noun_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notebook gave an error and asked to download 'wordnet'\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Twitter Messages to Be Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without stemming and lemmazing\n",
    "apple_processed_tweets = process_tweets(apple_df.tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of Words V.S. TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "bow_vectorizer = CountVectorizer(stop_words='english')\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "bow = bow_vectorizer.fit_transform(apple_processed_tweets)\n",
    "tfidf = tfidf_vectorizer.fit_transform(apple_processed_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Target Variables to be Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "di = {'Negative emotion':0, 'Positive emotion':1}\n",
    "y = apple_df.emotion.map(di)\n",
    "y.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW & Mutlinomial: The accuracy score for the training set is 0.949\n",
      "BOW & Mutlinomial: The accuracy score for the test set is 0.859\n"
     ]
    }
   ],
   "source": [
    "# spli data into training set and test tset\n",
    "bow_X_train, bow_X_test, bow_y_train, bow_y_test = train_test_split(bow,y)\n",
    "\n",
    "\n",
    "mnb_bow = MultinomialNB()\n",
    "mnb_bow.fit(bow_X_train,bow_y_train)\n",
    "\n",
    "#mnb_bow.predict(g_bowX)\n",
    "print('BOW & Mutlinomial: The accuracy score for the training set is {0:.3f}'.format(mnb_bow.score(bow_X_train,bow_y_train)))\n",
    "print('BOW & Mutlinomial: The accuracy score for the test set is {0:.3f}'.format(mnb_bow.score(bow_X_test, bow_y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF & Mutlinomial: The accuracy score for the training set is 0.860\n",
      "TF_IDF & Mutlinomial: The accuracy score for the test set is 0.813\n"
     ]
    }
   ],
   "source": [
    "# Use TF-IDF\n",
    "tf_X_train, tf_X_test, tf_y_train, tf_y_test = train_test_split(tfidf,y)\n",
    "\n",
    "\n",
    "mnb_tfidf = MultinomialNB()\n",
    "mnb_tfidf.fit(tf_X_train,tf_y_train)\n",
    "\n",
    "print('TF-IDF & Mutlinomial: The accuracy score for the training set is {0:.3f}'.format(mnb_tfidf.score(tf_X_train,tf_y_train)))\n",
    "print('TF_IDF & Mutlinomial: The accuracy score for the test set is {0:.3f}'.format(mnb_tfidf.score(tf_X_test, tf_y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Multinomial Navie Bayes Algorithm, BOW Works out with Better Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Differemt Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW & SVM: The accuracy score for the training set is 0.836\n",
      "BOW & SVM: The accuracy score for the test set is 0.827\n",
      "\n",
      "TF-IDF & SVM: The accuracy score for the training set is 0.844\n",
      "TF-IDF & SVM: The accuracy score for the test set is 0.804\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC()\n",
    "clf.fit(bow_X_train,bow_y_train)\n",
    "print('BOW & SVM: The accuracy score for the training set is {0:.3f}'.format(clf.score(bow_X_train, bow_y_train)))\n",
    "print('BOW & SVM: The accuracy score for the test set is {0:.3f}'.format(clf.score(bow_X_test, bow_y_test)))\n",
    "print()\n",
    "\n",
    "clf = svm.SVC()\n",
    "clf.fit(tf_X_train,tf_y_train)\n",
    "print('TF-IDF & SVM: The accuracy score for the training set is {0:.3f}'.format(clf.score(tf_X_train,tf_y_train)))\n",
    "print('TF-IDF & SVM: The accuracy score for the test set is {0:.3f}'.format(clf.score(tf_X_test, tf_y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing different combinations of diffferent word representations and classifying algorithms, it turns out that BOW and Naive Bayes are the best combo to train the model. Next, I will do a hyperparameter tuning on the parameters of BOW and Multinomial Navie Bayes to further improve the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "done in 13.447\n",
      "\n",
      "Best score for training set: 0.867\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.5\n",
      "\tvect__binary: True\n",
      "\tvect__max_df: 0.8\n",
      "\tvect__min_df: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 240 out of 240 | elapsed:   13.3s finished\n"
     ]
    }
   ],
   "source": [
    "parameters = {'vect__max_df':[0.8,0.85,0.9,0.95], 'vect__min_df':[1,2],'vect__binary':[True,False],'clf__alpha':[0.25,0.5,0.75]}\n",
    "pipeline = Pipeline([('vect',CountVectorizer()),\n",
    "                    ('clf',MultinomialNB())])\n",
    "grid_search = GridSearchCV(pipeline, parameters,cv=5,\n",
    "                          n_jobs=1,verbose=1,scoring='accuracy')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(apple_processed_tweets,y)\n",
    "\n",
    "t0 = time()\n",
    "grid_search.fit(X_train,y_train)\n",
    "print('done in {0:0.3f}'.format((time() - t0)))\n",
    "print()\n",
    "\n",
    "print('Best score for training set: {0:0.3f}'.format(grid_search.best_score_))\n",
    "print('Best parameters set:')\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuarcy on test set is 0.890\n"
     ]
    }
   ],
   "source": [
    "# Predict on test set\n",
    "print('Accuarcy on test set is {0:0.3f}'.format(grid_search.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The best score is 0.867 on the training data set and 0.890 on the test data set. The test score is even a little bit better than the training’. It seems like a great result, but it is not really like that. I will explain why shortly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strongly Predictive Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I need to use the get_feature_names method from the CountVectorizer convector to get the actually words. An identity matrix is used to create a matrix that each row has exactly one word. Then, I use the trained MultinomialNB classifier to predict on this matrix. Finally, sort the rows by predicted probabilities, and pick the top and bottom 10 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(binary=grid_search.best_params_['vect__binary'],\n",
    "                      min_df=grid_search.best_params_['vect__min_df'],\n",
    "                      max_df=grid_search.best_params_['vect__max_df'])\n",
    "\n",
    "X = vec.fit_transform(apple_processed_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data: 0.968553\n",
      "Accuracy on test data:     0.825043\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "clf = MultinomialNB(alpha=grid_search.best_params_['clf__alpha']).fit(X_train,y_train)\n",
    "\n",
    "training_accuracy = clf.score(X_train, y_train)\n",
    "test_accuracy = clf.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy on training data: {0:2f}\".format(training_accuracy))\n",
    "print(\"Accuracy on test data:     {0:2f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 54  38]\n",
      " [ 64 427]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array(vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good words\t     P(positivity | word)\n",
      "              before 0.99\n",
      "                wins 0.99\n",
      "                 win 0.99\n",
      "              begins 0.99\n",
      "               smart 0.99\n",
      "             winning 0.99\n",
      "            downtown 0.98\n",
      "            download 0.98\n",
      "                 set 0.98\n",
      "                game 0.98\n",
      "Bad words\t     P(negativity | word)\n",
      "                suns 0.13\n",
      "         autocorrect 0.13\n",
      "             swisher 0.11\n",
      "             novelty 0.10\n",
      "           classiest 0.10\n",
      "           delegates 0.10\n",
      "               among 0.10\n",
      "               fades 0.08\n",
      "             fascist 0.07\n",
      "                hate 0.06\n"
     ]
    }
   ],
   "source": [
    "x = np.eye(X_test.shape[1])\n",
    "probs = clf.predict_log_proba(x)[:,0]\n",
    "ind = np.argsort(probs)\n",
    "\n",
    "good_words = features[ind[:10]]\n",
    "bad_words = features[ind[-10:]]\n",
    "\n",
    "good_prob = probs[ind[:10]]\n",
    "bad_prob = probs[ind[-10:]]\n",
    "\n",
    "print(\"Good words\\t     P(positivity | word)\")\n",
    "for w, p in zip(good_words, good_prob):\n",
    "    print(\"{:>20}\".format(w), \"{:.2f}\".format(1 - np.exp(p)))\n",
    "    \n",
    "print(\"Bad words\\t     P(negativity | word)\")\n",
    "for w, p in zip(bad_words, bad_prob):\n",
    "    print(\"{:>20}\".format(w), \"{:.2f}\".format(1 - np.exp(p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score on the google tweets is 0.753957\n"
     ]
    }
   ],
   "source": [
    "google_processed_tweets = process_tweets(google_df.tweets)\n",
    "google_x = vec.transform(google_processed_tweets)\n",
    "google_y = google_df.emotion.map(di)\n",
    "print('The accuracy score on the google tweets is {0:3f}'.format(\n",
    "    clf.score(google_x,google_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The outcomes are not exactly as good as when looking at the top 10 positive words, anyone will agree that they would think these words mean positive or negative. For instance, set, before, and begins are listed in the top 10 strongest predictive words, but when I think of them, I don't really have any emotion towards them. Some words in the weakest predictive words make few sense, such as classiest and novelty. These words definitely indicate positiveness to me. I am not sure why my model would learn that they are the top 10 negative words, unless they are used sarcastically very often in my data set.  Moreover, I notice that both win and wins appear in the top 10 strongest predictive words list, but clearly win and wins mean the same thing. Thus, I think I should try word stemming and lemmatization and see if that could help improve my model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to Use the Words Stems and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "stems = pd.Series(apple_processed_tweets).apply(stem_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data: 0.955975\n",
      "Accuracy on test data:     0.869640\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(binary=grid_search.best_params_['vect__binary'],\n",
    "                      min_df=grid_search.best_params_['vect__min_df'],\n",
    "                      max_df=grid_search.best_params_['vect__max_df'])\n",
    "stem_X = vec.fit_transform(stems)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(stem_X,y,random_state=38)\n",
    "clf = MultinomialNB(alpha=grid_search.best_params_['clf__alpha']).fit(X_train,y_train)\n",
    "\n",
    "training_accuracy = clf.score(X_train, y_train)\n",
    "test_accuracy = clf.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy on training data: {0:2f}\".format(training_accuracy))\n",
    "print(\"Accuracy on test data:     {0:2f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good words\t     P(positivity | word)\n",
      "                 win 1.00\n",
      "                 set 0.99\n",
      "               begin 0.99\n",
      "                 gam 0.98\n",
      "                  th 0.98\n",
      "            congress 0.98\n",
      "            downtown 0.98\n",
      "                play 0.98\n",
      "                 mus 0.98\n",
      "             tonight 0.98\n",
      "Bad words\t     P(negativity | word)\n",
      "                 kar 0.10\n",
      "               swish 0.09\n",
      "               among 0.08\n",
      "                fuck 0.08\n",
      "           classiest 0.08\n",
      "              iphone 0.08\n",
      "               deleg 0.08\n",
      "               novel 0.08\n",
      "                 fad 0.07\n",
      "                fasc 0.06\n"
     ]
    }
   ],
   "source": [
    "\n",
    "features = np.array(vec.get_feature_names())\n",
    "\n",
    "x = np.eye(X_test.shape[1])\n",
    "probs = clf.predict_log_proba(x)[:,0]\n",
    "ind = np.argsort(probs)\n",
    "\n",
    "good_words = features[ind[:10]]\n",
    "bad_words = features[ind[-10:]]\n",
    "\n",
    "good_prob = probs[ind[:10]]\n",
    "bad_prob = probs[ind[-10:]]\n",
    "\n",
    "print(\"Good words\\t     P(positivity | word)\")\n",
    "for w, p in zip(good_words, good_prob):\n",
    "    print(\"{:>20}\".format(w), \"{:.2f}\".format(1 - np.exp(p)))\n",
    "    \n",
    "print(\"Bad words\\t     P(negativity | word)\")\n",
    "for w, p in zip(bad_words, bad_prob):\n",
    "    print(\"{:>20}\".format(w), \"{:.2f}\".format(1 - np.exp(p)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Please note that there are some word stems in the above list do not seem like proper English words. That is what one may get when using the LancasterStemmer. LancasterStemmer tends to be heavy stemming, which leads stems to be non-linguistic. But it does give the best classification accuracy among my four models when you see the results of the rest models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_stems = pd.Series(google_processed_tweets).apply(stem_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score on the google tweets is 0.761151\n"
     ]
    }
   ],
   "source": [
    "print('The accuracy score on the google tweets is {0:3f}'.format(\n",
    "    clf.score(vec.transform(g_stems),google_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_lemma = [x[0] for x in pd.Series(apple_processed_tweets).apply(lemmatize_words)]\n",
    "noun_lemma = [x[1] for x in pd.Series(apple_processed_tweets).apply(lemmatize_words)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data: 0.959977\n",
      "Accuracy on test data:     0.842196\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(binary=grid_search.best_params_['vect__binary'],\n",
    "                      min_df=grid_search.best_params_['vect__min_df'],\n",
    "                      max_df=grid_search.best_params_['vect__max_df'])\n",
    "verb_lemma_X = vec.fit_transform(verb_lemma)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(verb_lemma_X,y,random_state=38)\n",
    "clf = MultinomialNB(alpha=grid_search.best_params_['clf__alpha']).fit(X_train,y_train)\n",
    "\n",
    "training_accuracy = clf.score(X_train, y_train)\n",
    "test_accuracy = clf.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy on training data: {0:2f}\".format(training_accuracy))\n",
    "print(\"Accuracy on test data:     {0:2f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_verb_lemma = [x[0] for x in pd.Series(google_processed_tweets).apply(lemmatize_words)]\n",
    "#g_noun_lemma = [x[1] for x in pd.Series(google_processed_tweets).apply(lemmatize_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score on the google tweets is 0.745324\n"
     ]
    }
   ],
   "source": [
    "print('The accuracy score on the google tweets is {0:3f}'.format(\n",
    "    clf.score(vec.transform(g_verb_lemma),google_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data: 0.965695\n",
      "Accuracy on test data:     0.869640\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(binary=grid_search.best_params_['vect__binary'],\n",
    "                      min_df=grid_search.best_params_['vect__min_df'],\n",
    "                      max_df=grid_search.best_params_['vect__max_df'])\n",
    "noun_lemma_X = vec.fit_transform(noun_lemma)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(noun_lemma_X,y,random_state=38)\n",
    "clf = MultinomialNB(alpha=grid_search.best_params_['clf__alpha']).fit(X_train,y_train)\n",
    "\n",
    "training_accuracy = clf.score(X_train, y_train)\n",
    "test_accuracy = clf.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy on training data: {0:2f}\".format(training_accuracy))\n",
    "print(\"Accuracy on test data:     {0:2f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good words\t     P(positivity | word)\n",
      "                 win 0.99\n",
      "                 set 0.99\n",
      "                wins 0.99\n",
      "              begins 0.99\n",
      "            download 0.98\n",
      "                  th 0.98\n",
      "             winning 0.98\n",
      "            congress 0.98\n",
      "                game 0.98\n",
      "            downtown 0.98\n",
      "Bad words\t     P(negativity | word)\n",
      "                kara 0.11\n",
      "            headache 0.11\n",
      "                hate 0.09\n",
      "             swisher 0.09\n",
      "           classiest 0.08\n",
      "             novelty 0.08\n",
      "               among 0.08\n",
      "            delegate 0.08\n",
      "                fade 0.07\n",
      "             fascist 0.06\n"
     ]
    }
   ],
   "source": [
    "features = np.array(vec.get_feature_names())\n",
    "\n",
    "x = np.eye(X_test.shape[1])\n",
    "probs = clf.predict_log_proba(x)[:,0]\n",
    "ind = np.argsort(probs)\n",
    "\n",
    "good_words = features[ind[:10]]\n",
    "bad_words = features[ind[-10:]]\n",
    "\n",
    "good_prob = probs[ind[:10]]\n",
    "bad_prob = probs[ind[-10:]]\n",
    "\n",
    "print(\"Good words\\t     P(positivity | word)\")\n",
    "for w, p in zip(good_words, good_prob):\n",
    "    print(\"{:>20}\".format(w), \"{:.2f}\".format(1 - np.exp(p)))\n",
    "    \n",
    "print(\"Bad words\\t     P(negativity | word)\")\n",
    "for w, p in zip(bad_words, bad_prob):\n",
    "    print(\"{:>20}\".format(w), \"{:.2f}\".format(1 - np.exp(p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g_verb_lemma = [x[0] for x in pd.Series(google_processed_tweets).apply(lemmatize_words)]\n",
    "g_noun_lemma = [x[1] for x in pd.Series(google_processed_tweets).apply(lemmatize_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score on the google tweets is 0.735252\n"
     ]
    }
   ],
   "source": [
    "print('The accuracy score on the google tweets is {0:3f}'.format(\n",
    "    clf.score(vec.transform(g_noun_lemma),google_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It looks like using word stems to train this  MultinomialNB classifier performs the best not only on the test Apple data, but also on the Google data. Thus, the model trained by the word stems will be my final model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
